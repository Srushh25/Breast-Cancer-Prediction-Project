# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pcXmwzYBuy7uMmW6FLIPJ_SZUotcsa44
"""

#downloading python modules
!pip install numpy
!pip install pandas
!pip install matplotlib
!pip install seaborn




#import the modules
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns




df = pd.read_csv('/content/data.csv')




df.head()




#EDA
#checking total no of rows and cols
df.shape




#checking cols and their corresponding data types
#properties of data - summary statistics
df.info()




#2nd way to check for null values
df.isnull().sum()



#drop column with all missing values
df =  df.dropna(axis=1)



df.shape



#checking data types
df.dtypes



df ['diagnosis'].value_counts()




sns.countplot(df['diagnosis'],label ='count')




from sklearn.preprocessing import LabelEncoder
LabelEncoder_Y = LabelEncoder()




#transform categorical to numerical
df.iloc[:,1]=LabelEncoder_Y.fit_transform(df.iloc[:,1].values)




#encoding
from sklearn.preprocessing import LabelEncoder
LabelEncoder_Y = LabelEncoder()





#transform categorical data to numerical
df.iloc[:,1]= LabelEncoder_Y.fit_transform(df.iloc[:,1].values)





df.iloc[:,1].values





sns.pairplot(df.iloc[:,1:7], hue="diagnosis")





#correlation between cols
df.iloc[:,1:11].corr()




#heatmap
plt.figure(figsize=(10,10))
sns.heatmap(df.iloc[:,1:11].corr(),cmap="YlGnBu",annot=True,fmt='.0%')




#feature scaling
#split our dataset into independent and dependent datasets
#independent --> x
#dependent --> y
X=df.iloc[:,2:31].values
Y=df.iloc[:,1].values




#80:20 ratio
from sklearn .model_selection import train_test_split
X_train, X_test, Y_train, Y_test =  train_test_split(X,Y,test_size=0.20,random_state=0)




from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train= sc.fit_transform(X_train)
X_test= sc.fit_transform(X_test)




X_train




def models(X_train,Y_train):
  from sklearn.linear_model import LogisticRegression
  log=LogisticRegression(random_state=0)
  log.fit(X_train,Y_train)

  from sklearn.tree import DecisionTreeClassifier
  tree=DecisionTreeClassifier(criterion='entropy',random_state=0)
  tree.fit(X_train,Y_train)

  from sklearn.ensemble import RandomForestClassifier
  forest=RandomForestClassifier(n_estimators=10, criterion='entropy',random_state=0)
  forest.fit(X_train,Y_train)

  #print accuracy of each model on the training dataset
  print('The accuracy of Logistic Regression : ',log.score(X_train,Y_train))
  print('The accuracy of Decision Tree : ',tree.score(X_train,Y_train))
  print('The accuracy of Random Forest : ',forest.score(X_train,Y_train))
  return log,tree,forest




model = models(X_train,Y_train)

from sklearn.metrics import confusion_matrix
cm =  confusion_matrix(Y_test,model[0].predict(X_test))
tp = cm[0][0]
tn = cm[1][1]
fn = cm[1][0]
fp = cm[0][1]
print('Accuracy : ',(tp+tn)/(tp+tn+fp+fn))




from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
for i in range(len(model)):
  print('Model : ',i)
  print(classification_report(Y_test,model[i].predict(X_test)))
  print(accuracy_score(Y_test,model[i].predict(X_test)))
  print()

  

#prediction
pred = model[2].predict(X_test)
print('Our model prediction : ')
print(pred)
print()
print('Actual prediction : ')
print(Y_test)